{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 5.3 - Curation Keywords: Remove Unused\n",
    "\n",
    "[From a comment to the MarketPlace Curation issue #1](https://github.com/SSHOC/marketplace-curation/issues/1):  \n",
    "...\n",
    "\n",
    "To solve this, we need another routine that does the cleanup of the sshoc-keyword-vocabulary concepts:\n",
    "\n",
    "- Go through all concepts of the vocabulary keywords => GET /api/vocabularies/sshoc-keyword (this is a case-sensitive list)\n",
    "- Look, if the keyword is not connected to any item => GET /api/item-search and search in the result of this call if in the object \"facets\": {\"keyword\": ...} the concept shows up (beware, case-sensitive)\n",
    "- If the concept is not there (it shows only concepts that are connected to one or more items) this means, we can delete it with calling DELETE /api/vocabularies/sshoc-keyword/concepts/{code} (the code needs to come from the first call) \n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 Requirements to run this notebook\n",
    "\n",
    "This section gives all the relevant information to \"interact\" with the MP data.\n",
    "\n",
    "### 0.1 libraries\n",
    "*There are a number of external libraries needed to run the notebook* \n",
    "\n",
    "*Furthermore, a dedicated SSH Open Marketplace library - sshmarketplacelib - with customised functions has been created and can be imported using the python import commands.* \n",
    "\n",
    "*Below the libraries import needed to run this notebook*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "#import the MarketPlace Library \n",
    "from sshmarketplacelib import MPData as mpd\n",
    "from sshmarketplacelib import  eval as eva, helper as hel\n",
    "import os\n",
    "import json\n",
    "import yaml\n",
    "import urllib.request\n",
    "import urllib\n",
    "import errno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1.1 Download data from configuration file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (os.path.isfile('config.yaml')):\n",
    "            configfile=\"config.yaml\"\n",
    "else:\n",
    "    raise FileNotFoundError(errno.ENOENT, os.strerror(errno.ENOENT), \"config.yaml\")\n",
    "try:\n",
    "    with open(configfile, 'r') as stream:\n",
    "        try:\n",
    "            conf=yaml.safe_load(stream)\n",
    "        except yaml.YAMLError as exc:\n",
    "            print(exc)\n",
    "except FileNotFoundError:\n",
    "    print('Warning config.yaml file not present! Please create it and set the values, store it in the main directory')\n",
    "debugMode=conf['DEBUG']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if debugMode:\n",
    "    print('Running in debug mode')\n",
    "else:\n",
    "    print('Running in production mode, updated data will be written back to the server')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMPSSHOCKeywords():\n",
    "    df_keywords = pd.DataFrame()\n",
    "    items= pd.DataFrame()\n",
    "    myurl=conf['API']['SERVER']+conf['DATASET_ENTRYPOINTS'][\"sshockeywords\"]\n",
    "        \n",
    "    with urllib.request.urlopen(myurl+'?perpage=20') as url:\n",
    "        df_desc_par = json.load(url)\n",
    "        \n",
    "    df_keywords= pd.DataFrame(df_desc_par[\"conceptResults\"]['concepts'])\n",
    "    df_keywords['var']=1\n",
    "    start=1\n",
    "        \n",
    "    if not df_keywords.empty:\n",
    "        pages=df_desc_par[\"conceptResults\"]['pages']\n",
    "        start+=1\n",
    "        \n",
    "        mdx = pd.Series(range(start, pages+1))\n",
    "        for var in mdx:\n",
    "            turl = myurl+\"?page=\"+str(var)\n",
    "            #print (f'{var}/{pages}, {df_keywords.shape[0]}, {turl+\"&perpage=20\"}')\n",
    "            try:\n",
    "                with urllib.request.urlopen(turl+'&perpage=20') as murl:\n",
    "                    #print (f'{var} - {turl+\"&perpage=100\"}')\n",
    "                    df_desc_par = json.load(murl)\n",
    "                    temp=pd.DataFrame(df_desc_par[\"conceptResults\"][\"concepts\"])\n",
    "                    temp['var']=var\n",
    "                    df_keywords=pd.concat([df_keywords, temp])\n",
    "                    #df_keywords=pd.concat([df_keywords, pd.DataFrame(df_desc_par[\"conceptResults\"][\"concepts\"])])\n",
    "            except:\n",
    "                print(f'SEVERE: Error getting keywords. (Error loading {turl})')\n",
    "    return df_keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 Get the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpdata = mpd()\n",
    "df_tool_flat =mpdata.getMPItems (\"toolsandservices\", True)\n",
    "df_publication_flat =mpdata.getMPItems (\"publications\", True)\n",
    "df_trainingmaterials_flat =mpdata.getMPItems (\"trainingmaterials\", True)\n",
    "df_workflows_flat =mpdata.getMPItems (\"workflows\", True)\n",
    "df_datasets_flat =mpdata.getMPItems (\"datasets\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_tool_flat[['description', 'persistentId']].to_csv(path_or_buf='ts_peiddescr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2 = df_tool_flat.pivot_table(index = ['persistentId'], aggfunc ='size')\n",
    "# df_2=df2.to_frame()\n",
    "# df_2.reset_index(inplace=True)\n",
    "# df_3=df_2.rename(columns={df_2.columns[1]:'test', df_2.columns[0]:'myid'})\n",
    "# df_3.sort_values(['test', 'myid'], ascending=False).tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils=hel.Util()\n",
    "resultfields=['persistentId', 'MPUrl', 'category', 'label', 'status', 'type.code', 'type.label', 'concept.code', 'concept.label', 'concept.uri', 'concept.vocabulary.scheme']\n",
    "udf_alprop=utils.getAllPropertiesBySources()\n",
    "udf_alprop=udf_alprop.loc[ : ,resultfields]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "udf_alprop.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# udf_alprop_nan=udf_alprop[udf_alprop['concept.code'].isnull()]\n",
    "# udf_alprop_nan.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "udf_alprop_new = udf_alprop.drop_duplicates(subset = ['concept.code'],keep = 'last').reset_index(drop = True)\n",
    "udf_alprop_new.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sshockeywords=getMPSSHOCKeywords()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sshockeywords.sort_values('code').head()\n",
    "df2 = df_sshockeywords.pivot_table(index = ['code'], aggfunc ='size')\n",
    "df_2=df2.to_frame()\n",
    "df_2.reset_index(inplace=True)\n",
    "df_3=df_2.rename(columns={df_2.columns[1]:'test', df_2.columns[0]:'myid'})\n",
    "df_3.sort_values(['myid', 'test'], ascending=False).iloc[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf = df_sshockeywords.drop_duplicates(subset = ['code'],keep = 'last').reset_index(drop = True)\n",
    "newdf.sort_values(['code']).iloc[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sshockeywords['mycode']=df_sshockeywords['code'].apply(lambda y: \" \".join(y.split()).lower().strip().replace('+',' '))\n",
    "newdf['mycode']=newdf['code'].apply(lambda y: \" \".join(y.split()).lower().strip().replace('+',' '))\n",
    "udf_alprop_new['mycode']=udf_alprop_new['concept.code'].apply(lambda y: \" \".join(str(y).split()).lower().strip().replace('+',' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unusedkeywords1=pd.DataFrame(columns = ['UnusedKW'])\n",
    "df_usedkeywords1=pd.DataFrame(columns = ['UsedKW'])\n",
    "for rown, row in newdf.iterrows():\n",
    "    temp_df=udf_alprop_new[udf_alprop_new['mycode']==row['mycode']]\n",
    "    #temp_df=udf_alprop_new[(udf_alprop_new['mycode']==row['mycode']) & (udf_alprop_new['status']=='approved')]\n",
    "    if(not len(temp_df)):\n",
    "        d = pd.DataFrame(data={'UnusedKW': [row.code]})\n",
    "        d['label'] = row.label\n",
    "        d['URI'] = row.uri\n",
    "        d['concept_code']=row['code']\n",
    "        df_unusedkeywords1=pd.concat([df_unusedkeywords1, d])\n",
    "    else:\n",
    "        #print (f'{row.code}, {temp_df[\"concept.code\"]}')\n",
    "        du = pd.DataFrame(data={'UsedKW': [row.code]})\n",
    "        du['label'] = row.label\n",
    "        du['URI'] = row.uri\n",
    "        du['itempid']=temp_df.iloc[0].persistentId\n",
    "        du['itemlabel']=temp_df.iloc[0].label\n",
    "        \n",
    "        df_usedkeywords1=pd.concat([df_usedkeywords1, du])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unusedkeywords1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unusedkeywords1.sort_values('UnusedKW').iloc[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unusedkeywords1.to_csv(path_or_buf='unusedkw1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_usedkeywords1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_usedkeywords1.to_csv(path_or_buf='usedkw.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.parse\n",
    "bearer=mpdata.checkCredentials()\n",
    "put_headers = {'Content-type': 'application/json', 'Authorization':bearer}\n",
    "df_log=pd.DataFrame()\n",
    "MPAPIserver=conf['API']['SERVER']\n",
    "for unkeyn, unkeyrow in df_unusedkeywords1.iterrows():\n",
    "    dl = pd.DataFrame(data={'UnusedKW': [unkeyrow.UnusedKW]})\n",
    "    deleteurl=f'{MPAPIserver}/api/vocabularies/sshoc-keyword/concepts/{urllib.parse.quote(unkeyrow.UnusedKW)}?force=true'\n",
    "    #deleteurl='https://marketplace-api.sshopencloud.eu/api/vocabularies/sshoc-keyword/concepts/'+urllib.parse.quote(unkeyrow.UnusedKW)+'?force=true'\n",
    "    print ('#############################')\n",
    "    print (f'{unkeyrow.UnusedKW}, {unkeyrow.concept_code}')\n",
    "   \n",
    "    print (f'{deleteurl} \\n')\n",
    "    print ('------- \\n')\n",
    "    if (not debugMode):\n",
    "        delete_result=requests.delete(deleteurl, headers=put_headers)\n",
    "        print (f'{delete_result.status_code}, {delete_result.text}')\n",
    "        if (delete_result.status_code==404):\n",
    "            print (' Error deleting keyword')\n",
    "        #dl['key'] = unkeyrow.UnusedKW\n",
    "        dl['DeleteResultCode'] = delete_result.status_code\n",
    "        dl['DeleteResultTest']= delete_result.text\n",
    "        df_log=pd.concat([df_log, dl])\n",
    "    else:\n",
    "        print( 'Running in debug mode, delete command not executed')\n",
    "\n",
    "# deleteurl='https://sshoc-marketplace-api-stage.acdh-dev.oeaw.ac.at/api/vocabularies/sshoc-keyword/concepts/AAH?force=true'\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_log.groupby('DeleteResultCode').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_log.to_csv(path_or_buf='conceptsdeletelogforce_9.csv')\n",
    "#df_log.to_pickle('conceptsdeletelog.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_log.sort_values('DeleteResultCode').tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
